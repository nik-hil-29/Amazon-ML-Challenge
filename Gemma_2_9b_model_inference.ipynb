{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAiorapbnkDe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
        "from torch import __version__; from packaging.version import Version as V\n",
        "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
        "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIYjwpiwoKfo"
      },
      "outputs": [],
      "source": [
        "# !pip install ninja\n",
        "# # Set TORCH_CUDA_ARCH_LIST if running and building on different GPU types\n",
        "# !pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUjp02wOoQSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50a10c3-0f24-467f-de3c-884d42fb166a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l-TP9lxoUFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c47b797-3b7a-412c-e89c-855447429f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.8 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/Lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Assuming you already have the test_df loaded\n",
        "# Drop the image_path column\n",
        "test_df = pd.read_csv('/content/Test_metadata.csv')\n",
        "test_df = test_df.drop(columns=['image_path'])\n",
        "\n",
        "# Split the data into X and y\n",
        "X = test_df"
      ],
      "metadata": {
        "id": "e_7A8Zh4skjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ico3lVNoYlN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6599a39d-d597-4173-8b43-3b0c2dd1a3cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3000it [59:40,  1.19s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "# Initialize a list to store the generated responses\n",
        "generated_responses = []\n",
        "\n",
        "# Loop through each row in X\n",
        "for idx, row in tqdm(X.iterrows()):\n",
        "    entity_name = row['entity_name']\n",
        "    extracted_text = row['extracted_text']\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt_text = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "Allowed Inputs:\n",
        "\n",
        "For the following entities, only the specified units are allowed. Please ensure that the input matches one of the permitted units for each entity:\n",
        "\n",
        "Width: centimetre, foot, inch, metre, millimetre, yard\n",
        "Depth: centimetre, foot, inch, metre, millimetre, yard\n",
        "Height: centimetre, foot, inch, metre, millimetre, yard\n",
        "Item Weight: gram, kilogram, microgram, milligram, ounce, pound, ton\n",
        "Maximum Weight Recommendation: gram, kilogram, microgram, milligram, ounce, pound, ton\n",
        "Voltage: kilovolt, millivolt, volt\n",
        "Wattage: kilowatt, watt\n",
        "Item Volume**: centilitre, cubic foot, cubic inch, cup, decilitre, fluid ounce, gallon, imperial gallon, litre, microlitre, millilitre, pint, quart\n",
        "\n",
        "Ensure that each input is correctly formatted and limited to the allowed units for each entity. Any deviation from these units will be considered invalid.\n",
        "\n",
        "### Instruction:\n",
        "Extract the {entity_name} from the text\n",
        "\n",
        "### Input:\n",
        "{extracted_text}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    # Tokenize the input for the model\n",
        "    inputs = tokenizer(\n",
        "        [prompt_text],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")  # Assuming the model is on CUDA\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "    # Decode the output\n",
        "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    # Extract the response part after the ### Response: prompt\n",
        "    response_start = \"### Response:\\n\"\n",
        "    response = decoded_output.split(response_start)[-1].strip(\"<eos>\").strip()\n",
        "\n",
        "    # Append the response to the list\n",
        "    generated_responses.append(response)\n",
        "\n",
        "# Store the responses in a DataFrame\n",
        "response_df = pd.DataFrame({\n",
        "      'prediction': generated_responses,\n",
        "})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_df.to_csv('inference_flie.csv', index = False)"
      ],
      "metadata": {
        "id": "8WimZ00M52T_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}